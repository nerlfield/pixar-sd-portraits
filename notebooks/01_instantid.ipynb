{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install unzip\n",
    "# !pip install opencv-python==4.10.0.84 insightface onnxruntime controlnet-aux gdown accelerate transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# import spaces\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from typing import Tuple\n",
    "\n",
    "import diffusers\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.models import ControlNetModel\n",
    "from diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# from style_template import styles\n",
    "from pipeline_stable_diffusion_xl_instantid_full import StableDiffusionXLInstantIDPipeline, draw_kps\n",
    "\n",
    "from controlnet_aux import OpenposeDetector\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from src.depth_anything.dpt import DepthAnything\n",
    "from src.depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda:5\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if str(device).__contains__(device) else torch.float32\n",
    "DEFAULT_STYLE_NAME = \"Spring Festival\"\n",
    "enable_lcm_arg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download antelopev2\n",
    "# !gdown 18wEUfMNohBJ4K3Ly5wpTejPfDzp-8fI8\n",
    "# !unzip ./antelopev2.zip\n",
    "# !mkdir models/\n",
    "# !mv ./antelopev2 ./models/\n",
    "# !cp -r InstantID/torchhub ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: ./models/antelopev2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: ./models/antelopev2/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: ./models/antelopev2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: ./models/antelopev2/glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: ./models/antelopev2/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dinov2:xFormers not available\n",
      "WARNING:dinov2:xFormers not available\n",
      "INFO:dinov2:using MLP layer as FFN\n",
      "An error occurred while trying to fetch thibaud/controlnet-openpose-sdxl-1.0: thibaud/controlnet-openpose-sdxl-1.0 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/config.json\", local_dir=\"./checkpoints\")\n",
    "hf_hub_download(\n",
    "    repo_id=\"InstantX/InstantID\",\n",
    "    filename=\"ControlNetModel/diffusion_pytorch_model.safetensors\",\n",
    "    local_dir=\"./checkpoints\",\n",
    ")\n",
    "hf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ip-adapter.bin\", local_dir=\"./checkpoints\")\n",
    "\n",
    "# Load face encoder\n",
    "app = FaceAnalysis(\n",
    "    name=\"antelopev2\",\n",
    "    root=\"./\",\n",
    "    providers=[\"CPUExecutionProvider\"],\n",
    ")\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "\n",
    "depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_vitl14').to(device).eval()\n",
    "\n",
    "transform = Compose([\n",
    "    Resize(\n",
    "        width=518,\n",
    "        height=518,\n",
    "        resize_target=False,\n",
    "        keep_aspect_ratio=True,\n",
    "        ensure_multiple_of=14,\n",
    "        resize_method='lower_bound',\n",
    "        image_interpolation_method=cv2.INTER_CUBIC,\n",
    "    ),\n",
    "    NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    PrepareForNet(),\n",
    "])\n",
    "\n",
    "# Path to InstantID models\n",
    "face_adapter = f\"./checkpoints/ip-adapter.bin\"\n",
    "controlnet_path = f\"./checkpoints/ControlNetModel\"\n",
    "\n",
    "# Load pipeline face ControlNetModel\n",
    "controlnet_identitynet = ControlNetModel.from_pretrained(\n",
    "    controlnet_path, torch_dtype=dtype\n",
    ")\n",
    "\n",
    "# controlnet-pose/canny/depth\n",
    "controlnet_pose_model = \"thibaud/controlnet-openpose-sdxl-1.0\"\n",
    "controlnet_canny_model = \"diffusers/controlnet-canny-sdxl-1.0\"\n",
    "controlnet_depth_model = \"diffusers/controlnet-depth-sdxl-1.0-small\"\n",
    "\n",
    "controlnet_pose = ControlNetModel.from_pretrained(\n",
    "    controlnet_pose_model, torch_dtype=dtype\n",
    ").to(device)\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    controlnet_canny_model, torch_dtype=dtype\n",
    ").to(device)\n",
    "controlnet_depth = ControlNetModel.from_pretrained(\n",
    "    controlnet_depth_model, torch_dtype=dtype\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'controlnet_list': ['controlnet', 'RPMultiControlNetModel'], 'requires_aesthetics_score': False} were passed to StableDiffusionXLInstantIDPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
      "Keyword arguments {'controlnet_list': ['controlnet', 'RPMultiControlNetModel'], 'requires_aesthetics_score': False, 'safety_checker': None} are not expected by StableDiffusionXLInstantIDPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  7.04steps/s]\n"
     ]
    }
   ],
   "source": [
    "def get_depth_map(image):\n",
    "    \n",
    "    image = np.array(image) / 255.0\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    image = transform({'image': image})['image']\n",
    "    image = torch.from_numpy(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        depth = depth_anything(image)\n",
    "\n",
    "    depth = F.interpolate(depth[None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "\n",
    "    depth = depth.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    depth_image = Image.fromarray(depth)\n",
    "\n",
    "    return depth_image\n",
    "\n",
    "def get_canny_image(image, t1=100, t2=200):\n",
    "    image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    edges = cv2.Canny(image, t1, t2)\n",
    "    return Image.fromarray(edges, \"L\")\n",
    "\n",
    "controlnet_map = {\n",
    "    \"pose\": controlnet_pose,\n",
    "    \"canny\": controlnet_canny,\n",
    "    \"depth\": controlnet_depth,\n",
    "}\n",
    "controlnet_map_fn = {\n",
    "    \"pose\": openpose,\n",
    "    \"canny\": get_canny_image,\n",
    "    \"depth\": get_depth_map,\n",
    "}\n",
    "\n",
    "pretrained_model_name_or_path = \"wangqixun/YamerMIX_v8\"\n",
    "# pretrained_model_name_or_path = \"/home/user/app/hentaiMixXLWorldIs_v10.safetensors\"\n",
    "\n",
    "# pipe = StableDiffusionXLInstantIDPipeline.from_single_file(pretrained_model_name_or_path, \n",
    "#                                                            torch_dtype=dtype, \n",
    "#                                                            controlnet = [controlnet_identitynet]).to(device)\n",
    "\n",
    "pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    controlnet=[controlnet_identitynet],\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "pipe.scheduler = diffusers.EulerDiscreteScheduler.from_config(\n",
    "    pipe.scheduler.config\n",
    ")\n",
    "\n",
    "# load and disable LCM\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n",
    "pipe.disable_lora()\n",
    "\n",
    "pipe.to(device)\n",
    "pipe.load_ip_adapter_instantid(face_adapter)\n",
    "pipe.image_proj_model.to(device)\n",
    "pipe.unet.to(device)\n",
    "\n",
    "style_list = [\n",
    "    {\n",
    "        \"name\": \"(No style)\",\n",
    "        \"prompt\": \"{prompt}\",\n",
    "        \"negative_prompt\": \"\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Spring Festival\",\n",
    "        \"prompt\": \"Flat illustration, a Chinese {prompt}, ancient style, wearing a red cloth, smile face, white skin, clean background, fireworks blooming, red lanterns\",\n",
    "        \"negative_prompt\": \"photo, deformed, black and white, realism, disfigured, low contrast, realistic, cropped, worst quality, missing fingers, extra digit, jpeg artifacts, signature, multiple, (lowres, low quality, worst quality:1.2)\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Watercolor\",\n",
    "        \"prompt\": \"watercolor painting, {prompt}. vibrant, beautiful, painterly, detailed, textural, artistic\",\n",
    "        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, anime, photorealistic, 35mm film, deformed, glitch, low contrast, noisy\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Film Noir\",\n",
    "        \"prompt\": \"film noir style, ink sketch|vector, {prompt} highly detailed, sharp focus, ultra sharpness, monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic\",\n",
    "        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Neon\",\n",
    "        \"prompt\": \"masterpiece painting, buildings in the backdrop, kaleidoscope, lilac orange blue cream fuchsia bright vivid gradient colors, the scene is cinematic, {prompt}, emotional realism, double exposure, watercolor ink pencil, graded wash, color layering, magic realism, figurative painting, intricate motifs, organic tracery, polished\",\n",
    "        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Jungle\",\n",
    "        \"prompt\": 'waist-up \"{prompt} in a Jungle\" by Syd Mead, tangerine cold color palette, muted colors, detailed, 8k,photo r3al,dripping paint,3d toon style,3d style,Movie Still',\n",
    "        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Mars\",\n",
    "        \"prompt\": \"{prompt}, Post-apocalyptic. Mars Colony, Scavengers roam the wastelands searching for valuable resources, rovers, bright morning sunlight shining, (detailed) (intricate) (8k) (HDR) (cinematic lighting) (sharp focus)\",\n",
    "        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Vibrant Color\",\n",
    "        \"prompt\": \"vibrant colorful, ink sketch|vector|2d colors, at nightfall, sharp focus, {prompt}, highly detailed, sharp focus, the clouds,colorful,ultra sharpness\",\n",
    "        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Snow\",\n",
    "        \"prompt\": \"cinema 4d render, {prompt}, high contrast, vibrant and saturated, sico style, surrounded by magical glow,floating ice shards, snow crystals, cold, windy background, frozen natural landscape in background  cinematic atmosphere,highly detailed, sharp focus, intricate design, 3d, unreal engine, octane render, CG best quality, highres, photorealistic, dramatic lighting, artstation, concept art, cinematic, epic Steven Spielberg movie still, sharp focus, smoke, sparks, art by pascal blanche and greg rutkowski and repin, trending on artstation, hyperrealism painting, matte painting, 4k resolution\",\n",
    "        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Line art\",\n",
    "        \"prompt\": \"line art drawing {prompt} . professional, sleek, modern, minimalist, graphic, line art, vector graphics\",\n",
    "        \"negative_prompt\": \"anime, photorealistic, 35mm film, deformed, glitch, blurry, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, mutated, realism, realistic, impressionism, expressionism, oil, acrylic\",\n",
    "    },\n",
    "]\n",
    "\n",
    "styles = {k[\"name\"]: (k[\"prompt\"], k[\"negative_prompt\"]) for k in style_list}\n",
    "\n",
    "def toggle_lcm_ui(value):\n",
    "    if value:\n",
    "        return (\n",
    "            gr.update(minimum=0, maximum=100, step=1, value=5),\n",
    "            gr.update(minimum=0.1, maximum=20.0, step=0.1, value=1.5),\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            gr.update(minimum=5, maximum=100, step=1, value=30),\n",
    "            gr.update(minimum=0.1, maximum=20.0, step=0.1, value=5),\n",
    "        )\n",
    "\n",
    "def randomize_seed_fn(seed: int, randomize_seed: bool) -> int:\n",
    "    if randomize_seed:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "    return seed\n",
    "\n",
    "def remove_tips():\n",
    "    return gr.update(visible=False)\n",
    "\n",
    "def get_example():\n",
    "    case = [\n",
    "        [\n",
    "            \"./examples/yann-lecun_resize.jpg\",\n",
    "            None,\n",
    "            \"a man\",\n",
    "            \"Spring Festival\",\n",
    "            \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "        ],\n",
    "        [\n",
    "            \"./examples/musk_resize.jpeg\",\n",
    "            \"./examples/poses/pose2.jpg\",\n",
    "            \"a man flying in the sky in Mars\",\n",
    "            \"Mars\",\n",
    "            \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "        ],\n",
    "        [\n",
    "            \"./examples/sam_resize.png\",\n",
    "            \"./examples/poses/pose4.jpg\",\n",
    "            \"a man doing a silly pose wearing a suite\",\n",
    "            \"Jungle\",\n",
    "            \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, gree\",\n",
    "        ],\n",
    "        [\n",
    "            \"./examples/schmidhuber_resize.png\",\n",
    "            \"./examples/poses/pose3.jpg\",\n",
    "            \"a man sit on a chair\",\n",
    "            \"Neon\",\n",
    "            \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "        ],\n",
    "        [\n",
    "            \"./examples/kaifu_resize.png\",\n",
    "            \"./examples/poses/pose.jpg\",\n",
    "            \"a man\",\n",
    "            \"Vibrant Color\",\n",
    "            \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n",
    "        ],\n",
    "    ]\n",
    "    return case[:1]\n",
    "\n",
    "def run_for_examples(face_file, pose_file, prompt, style, negative_prompt):\n",
    "    return generate_image(\n",
    "        face_file,\n",
    "        pose_file,\n",
    "        prompt,\n",
    "        negative_prompt,\n",
    "        style,\n",
    "        20,  # num_steps\n",
    "        0.8,  # identitynet_strength_ratio\n",
    "        0.8,  # adapter_strength_ratio\n",
    "        0.4,  # pose_strength\n",
    "        0.3,  # canny_strength\n",
    "        0.5,  # depth_strength\n",
    "        [\"pose\", \"canny\"],  # controlnet_selection\n",
    "        5.0,  # guidance_scale\n",
    "        42,  # seed\n",
    "        \"EulerDiscreteScheduler\",  # scheduler\n",
    "        False,  # enable_LCM\n",
    "        True,  # enable_Face_Region\n",
    "    )\n",
    "\n",
    "def convert_from_cv2_to_image(img: np.ndarray) -> Image:\n",
    "    return Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def convert_from_image_to_cv2(img: Image) -> np.ndarray:\n",
    "    return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def resize_img(\n",
    "    input_image,\n",
    "    max_side=1280,\n",
    "    min_side=1024,\n",
    "    size=None,\n",
    "    pad_to_max_side=False,\n",
    "    mode=PIL.Image.BILINEAR,\n",
    "    base_pixel_number=64,\n",
    "):\n",
    "    w, h = input_image.size\n",
    "    if size is not None:\n",
    "        w_resize_new, h_resize_new = size\n",
    "    else:\n",
    "        ratio = min_side / min(h, w)\n",
    "        w, h = round(ratio * w), round(ratio * h)\n",
    "        ratio = max_side / max(h, w)\n",
    "        input_image = input_image.resize([round(ratio * w), round(ratio * h)], mode)\n",
    "        w_resize_new = (round(ratio * w) // base_pixel_number) * base_pixel_number\n",
    "        h_resize_new = (round(ratio * h) // base_pixel_number) * base_pixel_number\n",
    "    input_image = input_image.resize([w_resize_new, h_resize_new], mode)\n",
    "\n",
    "    if pad_to_max_side:\n",
    "        res = np.ones([max_side, max_side, 3], dtype=np.uint8) * 255\n",
    "        offset_x = (max_side - w_resize_new) // 2\n",
    "        offset_y = (max_side - h_resize_new) // 2\n",
    "        res[\n",
    "            offset_y : offset_y + h_resize_new, offset_x : offset_x + w_resize_new\n",
    "        ] = np.array(input_image)\n",
    "        input_image = Image.fromarray(res)\n",
    "    return input_image\n",
    "\n",
    "def apply_style(\n",
    "    style_name: str, positive: str, negative: str = \"\"\n",
    ") -> Tuple[str, str]:\n",
    "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
    "    return p.replace(\"{prompt}\", positive), n + \" \" + negative\n",
    "\n",
    "# @spaces.GPU\n",
    "def generate_image(\n",
    "    face_image_path,\n",
    "    pose_image_path,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    style_name,\n",
    "    num_steps,\n",
    "    identitynet_strength_ratio,\n",
    "    adapter_strength_ratio,\n",
    "    pose_strength,\n",
    "    canny_strength,\n",
    "    depth_strength,\n",
    "    controlnet_selection,\n",
    "    guidance_scale,\n",
    "    seed,\n",
    "    scheduler,\n",
    "    enable_LCM,\n",
    "    enhance_face_region,\n",
    "    progress=gr.Progress(track_tqdm=True),\n",
    "):\n",
    "\n",
    "    if enable_LCM:\n",
    "        pipe.scheduler = diffusers.LCMScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.enable_lora()\n",
    "    else:\n",
    "        pipe.disable_lora()\n",
    "        scheduler_class_name = scheduler.split(\"-\")[0]\n",
    "\n",
    "        add_kwargs = {}\n",
    "        if len(scheduler.split(\"-\")) > 1:\n",
    "            add_kwargs[\"use_karras_sigmas\"] = True\n",
    "        if len(scheduler.split(\"-\")) > 2:\n",
    "            add_kwargs[\"algorithm_type\"] = \"sde-dpmsolver++\"\n",
    "        scheduler = getattr(diffusers, scheduler_class_name)\n",
    "        pipe.scheduler = scheduler.from_config(pipe.scheduler.config, **add_kwargs)\n",
    "\n",
    "    if face_image_path is None:\n",
    "        raise gr.Error(\n",
    "            f\"Cannot find any input face image! Please upload the face image\"\n",
    "        )\n",
    "\n",
    "    if prompt is None:\n",
    "        prompt = \"a person\"\n",
    "\n",
    "    # apply the style template\n",
    "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
    "\n",
    "    face_image = load_image(face_image_path)\n",
    "    face_image = resize_img(face_image, max_side=1024)\n",
    "    face_image_cv2 = convert_from_image_to_cv2(face_image)\n",
    "    height, width, _ = face_image_cv2.shape\n",
    "\n",
    "    # Extract face features\n",
    "    face_info = app.get(face_image_cv2)\n",
    "\n",
    "    if len(face_info) == 0:\n",
    "        raise gr.Error(\n",
    "            f\"Unable to detect a face in the image. Please upload a different photo with a clear face.\"\n",
    "        )\n",
    "\n",
    "    face_info = sorted(\n",
    "        face_info,\n",
    "        key=lambda x: (x[\"bbox\"][2] - x[\"bbox\"][0]) * x[\"bbox\"][3] - x[\"bbox\"][1],\n",
    "    )[\n",
    "        -1\n",
    "    ]  # only use the maximum face\n",
    "    face_emb = face_info[\"embedding\"]\n",
    "    face_kps = draw_kps(convert_from_cv2_to_image(face_image_cv2), face_info[\"kps\"])\n",
    "    img_controlnet = face_image\n",
    "    if pose_image_path is not None:\n",
    "        pose_image = load_image(pose_image_path)\n",
    "        pose_image = resize_img(pose_image, max_side=1024)\n",
    "        img_controlnet = pose_image\n",
    "        pose_image_cv2 = convert_from_image_to_cv2(pose_image)\n",
    "\n",
    "        face_info = app.get(pose_image_cv2)\n",
    "\n",
    "        if len(face_info) == 0:\n",
    "            raise gr.Error(\n",
    "                f\"Cannot find any face in the reference image! Please upload another person image\"\n",
    "            )\n",
    "\n",
    "        face_info = face_info[-1]\n",
    "        face_kps = draw_kps(pose_image, face_info[\"kps\"])\n",
    "\n",
    "        width, height = face_kps.size\n",
    "\n",
    "    if enhance_face_region:\n",
    "        control_mask = np.zeros([height, width, 3])\n",
    "        x1, y1, x2, y2 = face_info[\"bbox\"]\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        control_mask[y1:y2, x1:x2] = 255\n",
    "        control_mask = Image.fromarray(control_mask.astype(np.uint8))\n",
    "    else:\n",
    "        control_mask = None\n",
    "\n",
    "    if len(controlnet_selection) > 0:\n",
    "        controlnet_scales = {\n",
    "            \"pose\": pose_strength,\n",
    "            \"canny\": canny_strength,\n",
    "            \"depth\": depth_strength,\n",
    "        }\n",
    "        pipe.controlnet = MultiControlNetModel(\n",
    "            [controlnet_identitynet]\n",
    "            + [controlnet_map[s] for s in controlnet_selection]\n",
    "        )\n",
    "        control_scales = [float(identitynet_strength_ratio)] + [\n",
    "            controlnet_scales[s] for s in controlnet_selection\n",
    "        ]\n",
    "        control_images = [face_kps] + [\n",
    "            controlnet_map_fn[s](img_controlnet).resize((width, height))\n",
    "            for s in controlnet_selection\n",
    "        ]\n",
    "    else:\n",
    "        pipe.controlnet = controlnet_identitynet\n",
    "        control_scales = float(identitynet_strength_ratio)\n",
    "        control_images = face_kps\n",
    "\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    print(\"Start inference...\")\n",
    "    print(f\"[Debug] Prompt: {prompt}, \\n[Debug] Neg Prompt: {negative_prompt}\")\n",
    "\n",
    "    pipe.set_ip_adapter_scale(adapter_strength_ratio)\n",
    "    images = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image_embeds=face_emb,\n",
    "        image=control_images,\n",
    "        control_mask=control_mask,\n",
    "        controlnet_conditioning_scale=control_scales,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        generator=generator,\n",
    "        control_guidance_start=[0,0,0]\n",
    "    ).images\n",
    "\n",
    "    return images[0], gr.update(visible=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielko/miniconda3/envs/pixarsd/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inference...\n",
      "[Debug] Prompt: 1girl, naked, yoga, anime, sexy, watercolors, nude, intricate detail, extremely detailed, (masterpiece), best quality, highres, 4k, 8k, amazing quality, amazing shading, soft lighting, \n",
      "[Debug] Neg Prompt:  low resolution, monochrome, bad quality, deformed, bad anatomy, ugly, extra limbs,extra nipples, text, cartoon, 3d render, photorealistic, volume\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`control_guidance_start`: ['latents'] has 1 elements but there are 3 controlnets available. Make sure to provide 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m negative_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow resolution, monochrome, bad quality, deformed, bad anatomy, ugly, extra limbs,extra nipples, text, cartoon, 3d render, photorealistic, volume\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m style \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(No style)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m image, cf \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mface_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# num_steps\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# identitynet_strength_ratio\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# adapter_strength_ratio\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pose_strength\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# canny_strength\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# depth_strength\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcanny\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# controlnet_selection\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# guidance_scale\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# seed\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEulerDiscreteScheduler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# scheduler\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# enable_LCM\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# enable_Face_Region\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# control_guidance_start = [0,0]\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# image, cf = generate_image(\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#         face_file,\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#         pose_file,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#         # enhance_face_region=False,\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m     51\u001b[0m image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/res.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 371\u001b[0m, in \u001b[0;36mgenerate_image\u001b[0;34m(face_image_path, pose_image_path, prompt, negative_prompt, style_name, num_steps, identitynet_strength_ratio, adapter_strength_ratio, pose_strength, canny_strength, depth_strength, controlnet_selection, guidance_scale, seed, scheduler, enable_LCM, enhance_face_region, progress)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Debug] Prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Debug] Neg Prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnegative_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    370\u001b[0m pipe\u001b[38;5;241m.\u001b[39mset_ip_adapter_scale(adapter_strength_ratio)\n\u001b[0;32m--> 371\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mface_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_scales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_guidance_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images[\u001b[38;5;241m0\u001b[39m], gr\u001b[38;5;241m.\u001b[39mupdate(visible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pixarsd/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/softpear-rnd/pixar-sd-portraits/notebooks/pipeline_stable_diffusion_xl_instantid_full.py:838\u001b[0m, in \u001b[0;36mStableDiffusionXLInstantIDPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, image, height, width, num_inference_steps, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, image_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, ip_adapter_scale, control_mask, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_ip_adapter_scale(ip_adapter_scale)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# 1. Check inputs. Raise error if not correct\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_pooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_guidance_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_guidance_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_on_step_end_tensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guidance_scale \u001b[38;5;241m=\u001b[39m guidance_scale\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_skip \u001b[38;5;241m=\u001b[39m clip_skip\n",
      "File \u001b[0;32m~/miniconda3/envs/pixarsd/lib/python3.10/site-packages/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py:785\u001b[0m, in \u001b[0;36mStableDiffusionXLControlNetPipeline.check_inputs\u001b[0;34m(self, prompt, prompt_2, image, callback_steps, negative_prompt, negative_prompt_2, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, negative_pooled_prompt_embeds, controlnet_conditioning_scale, control_guidance_start, control_guidance_end, callback_on_step_end_tensor_inputs)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet, MultiControlNetModel):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(control_guidance_start) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet\u001b[38;5;241m.\u001b[39mnets):\n\u001b[0;32m--> 785\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    786\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`control_guidance_start`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontrol_guidance_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(control_guidance_start)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements but there are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet\u001b[38;5;241m.\u001b[39mnets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m controlnets available. Make sure to provide \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet\u001b[38;5;241m.\u001b[39mnets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    787\u001b[0m         )\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(control_guidance_start, control_guidance_end):\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end:\n",
      "\u001b[0;31mValueError\u001b[0m: `control_guidance_start`: ['latents'] has 1 elements but there are 3 controlnets available. Make sure to provide 3."
     ]
    }
   ],
   "source": [
    "face_file = 'images/clark.jpg'\n",
    "pose_file = 'images/girl.jpeg'\n",
    "\n",
    "# prompt = \"dark fantasy art by rebecca guay, dramatic pose, sexy, watercolors, nude, intricate detail, extremely detailed, gloomy colors, dark aura, horror, (masterpiece), best quality, highres, 4k, 8k, amazing quality, amazing shading, soft lighting\"\n",
    "prompt = \"1girl, naked, yoga, anime, sexy, watercolors, nude, intricate detail, extremely detailed, (masterpiece), best quality, highres, 4k, 8k, amazing quality, amazing shading, soft lighting\"\n",
    "negative_prompt = \"low resolution, monochrome, bad quality, deformed, bad anatomy, ugly, extra limbs,extra nipples, text, cartoon, 3d render, photorealistic, volume\"\n",
    "style = '(No style)'\n",
    "image, cf = generate_image(\n",
    "        face_file,\n",
    "        pose_file,\n",
    "        prompt,\n",
    "        negative_prompt,\n",
    "        style,\n",
    "        20,  # num_steps\n",
    "        0.8,  # identitynet_strength_ratio\n",
    "        0.8,  # adapter_strength_ratio\n",
    "        0.4,  # pose_strength\n",
    "        0.3,  # canny_strength\n",
    "        0.5,  # depth_strength\n",
    "        [\"pose\", \"canny\"],  # controlnet_selection\n",
    "        5.0,  # guidance_scale\n",
    "        42,  # seed\n",
    "        \"EulerDiscreteScheduler\",  # scheduler\n",
    "        False,  # enable_LCM\n",
    "        True,  # enable_Face_Region\n",
    "        # control_guidance_start = [0,0]\n",
    "    )\n",
    "\n",
    "\n",
    "# image, cf = generate_image(\n",
    "#         face_file,\n",
    "#         pose_file,\n",
    "#         prompt,\n",
    "#         negative_prompt,\n",
    "#         style,\n",
    "#         20,  # num_steps\n",
    "#         0.8,  # identitynet_strength_ratio\n",
    "#         0.2,  # adapter_strength_ratio\n",
    "#         0.8,  # pose_strength\n",
    "#         0.2,  # canny_strength\n",
    "#         0.2,  # depth_strength\n",
    "#         [],# [\"pose\", \"canny\"],#, \"depth\"],  # controlnet_selection\n",
    "#         7.0,  # guidance_scale\n",
    "#         42,  # seed\n",
    "#         \"DPMSolverMultistepScheduler-Karras\", #\"EulerDiscreteScheduler\",  # scheduler\n",
    "#         False,  # enable_LCM\n",
    "#         True,  # enable_Face_Region\n",
    "#         # enhance_face_region=False,\n",
    "#     )\n",
    "\n",
    "image.save('images/res.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe.controlnet.nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixarsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
